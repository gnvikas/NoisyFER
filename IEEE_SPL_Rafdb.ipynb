{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnvikas/NoisyFER/blob/main/IEEE_SPL_Rafdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Owner: Vikas G N, gnvikas@gmail.com"
      ],
      "metadata": {
        "id": "QQiLbDAPjtfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains dataloader for Rafdb class and implementation of the work titled Instance Discrimination based Robust Training for Facial Expression Recognition under Noisy labels.\n"
      ],
      "metadata": {
        "id": "-lGhsW1sjhcb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyh_SSHqVD8E",
        "outputId": "0db07761-0031-4e24-e2ec-64a95e4c1420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lF7_sbSBChaL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import torch.optim\n",
        "import os\n",
        "import torch.utils.data as data\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "\n",
        "#logfile = open('/content/drive/MyDrive/Colab_Notebooks/mtech/Project/Logs/IDN/RAFDB/log-newloss.txt','w')\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "'''\n",
        "Aum Sri Sai Ram\n",
        "\n",
        "Resnet models\n",
        "\n",
        "                          NOTE: only layers required are retained and fine-tuned.\n",
        "\n",
        "'''\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out + residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=7, end2end=True):\n",
        "        self.inplanes = 64\n",
        "        self.end2end = end2end\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        bs = x.size(0)\n",
        "        f = x\n",
        "\n",
        "        f = self.conv1(f)\n",
        "        f = self.bn1(f)\n",
        "        f = self.relu(f)\n",
        "        f = self.maxpool(f)\n",
        "        \n",
        "        f = self.layer1(f)\n",
        "        #print('layer1: ',f.size())\n",
        "        f = self.layer2(f)\n",
        "        #print('layer2: ',f.size())\n",
        "        f = self.layer3(f)\n",
        "        feature = f.view(bs, -1)\n",
        "        #print('layer4: ',f.size())\n",
        "        f = self.layer4(f)\n",
        "        #print('layer4: ',f.size())\n",
        "        f = self.avgpool(f)\n",
        "        \n",
        "        f = f.squeeze(3).squeeze(2)\n",
        "        #return f\n",
        "        return  F.normalize(f) #f\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False,  **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    #if pretrained:\n",
        "     #   model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model\n",
        "    \n",
        "#-----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def load_base_model(model): #load pretrained MSCeleb-1M     \n",
        "   checkpoint = torch.load('/content/drive/MyDrive/Colab_Notebooks/mtech/Project/ijba_res18_naive.pth.tar')\n",
        "   pretrained_state_dict = checkpoint['state_dict']\n",
        "   model_state_dict = model.state_dict()\n",
        "   for key in pretrained_state_dict:\n",
        "       if  ((key == 'module.fc.weight') | (key=='module.fc.bias') | (key=='module.feature.weight') | (key=='module.feature.bias') ) :    \n",
        "           pass\n",
        "       else:           \n",
        "           model_state_dict[key] = pretrained_state_dict[key]\n",
        "\n",
        "   model.load_state_dict(model_state_dict, strict = False)\n",
        "   return model\n",
        "   \n",
        "class Classifier(nn.Module):\n",
        "      def __init__(self, input_dim = 512, num_classes = 7):\n",
        "          super(Classifier, self).__init__()\n",
        "          self.fc = nn.Linear(input_dim, num_classes)\n",
        "          \n",
        "      def forward(self, x):\n",
        "          out = self.fc(x)\n",
        "          probs = F.softmax(out, dim=1)\n",
        "          return out, probs\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)   \n",
        "    \n",
        "\n",
        "def instantiate_model(args):\n",
        "    \n",
        "    base_model = resnet18(pretrained=False) \n",
        "    base_model = nn.DataParallel(base_model).to(args.device)\n",
        "    base_model = load_base_model(base_model)\n",
        "    \n",
        "    src_cl1 =  Classifier(num_classes = args.num_src_classes).to(args.device)\n",
        "    src_cl2 =  Classifier(num_classes = args.num_src_classes).to(args.device)\n",
        "    ins_cl =  Classifier(num_classes = args.num_ins_classes).to(args.device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss(reduction = 'none').to(args.device)\n",
        "    \n",
        "    criterion_kl = nn.KLDivLoss().to(args.device)\n",
        "    \n",
        "    optimizer = torch.optim.Adam([{'params':base_model.parameters(), 'lr': args.base_model_lr, 'weigh_decay' : args.base_model_wd},\n",
        "                                 {'params':src_cl1.parameters(), 'lr': args.src_lr, 'weigh_decay' : args.other_wd},\n",
        "                                 {'params':src_cl2.parameters(), 'lr': args.src_lr, 'weigh_decay' : args.other_wd},\n",
        "                                 {'params':ins_cl.parameters(), 'lr': args.ins_lr, 'weigh_decay' : args.other_wd}  \n",
        "                                ]#, momentum = args.momentum, nesterov = True\n",
        "                               )\n",
        "                                \n",
        "    \n",
        "    return base_model, src_cl1, src_cl2, ins_cl, criterion, criterion_kl, optimizer\n",
        "    \n",
        "def adjust_learning_rate(optimizer): \n",
        "  for param_group in optimizer.param_groups: \n",
        "      param_group[\"lr\"] /= 10.\n",
        "    \n",
        "    \n",
        "def train(args, train_dataset, test_dataset, logfile):    \n",
        "    model, src_cl1, src_cl2, ins_cl, criterion, criterion_kl, optimizer = instantiate_model(args)\n",
        "    src_cl1.train()\n",
        "    src_cl2.train()\n",
        "    ins_cl.train()\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = args.batch_size, drop_last = True, \n",
        "    \t\t\t\t\t\t\tnum_workers = args.num_workers, shuffle = True)\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = args.batch_size, num_workers = args.num_workers, shuffle = False)\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    count = 0.0\n",
        "    \n",
        "    for epoch in range(0, args.epochs):\n",
        "      \n",
        "      train_acc = count / len(train_dataset)\n",
        "      #print(f'epoch no: {epoch}, train_acc:{train_acc}',file=logfile)\n",
        "      #print(f'epoch no: {epoch}, train_acc:{train_acc}')\n",
        "      \n",
        "      count = 0.0\n",
        "      if len(train_dataset.clean_data) == len(train_dataset):\n",
        "          print(\"setting back to phase1\",file=logfile)\n",
        "          train_dataset.set_phase(1)\n",
        "      \n",
        "      #if epoch == 25 or epoch == 40 or epoch == 50: #fplus\n",
        "      if epoch == 20 or epoch == 28 or epoch==36:   #rafdb\n",
        "        adjust_learning_rate(optimizer)\n",
        "          \n",
        "      for i, (data1, data2, label1, label2, idx1, idx2, is_labeled1, is_labeled2) in enumerate(train_loader): #training\n",
        "          correct_cls_1, correct_cls_2, correct_ins  = 0., 0., 0.\n",
        "                    \n",
        "          data1 = data1.to(args.device)\n",
        "          label1 =  label1.to(args.device)\n",
        "          feat1 = model(data1)\n",
        "          idx1 = idx1.to(args.device)\n",
        "          out_src_cl1_1, probs_src_cl1_1 = src_cl1(feat1)\n",
        "          out_src_cl2_1, probs_src_cl2_1 = src_cl2(feat1)\n",
        "          out_ins_cl_1, probs_ins_cl_1 = ins_cl(feat1)\n",
        "          \n",
        "          \n",
        "          \n",
        "          if train_dataset.phase == 1 or epoch < args.warmup_epochs:\n",
        "             probs1, preds1 = torch.max(probs_src_cl1_1, dim = 1)\n",
        "             probs2, preds2 = torch.max(probs_src_cl2_1, dim = 1)\n",
        "             indices1 = ((preds1 == label1) & (preds2 == label1) & (probs1 > args.probs_threshold_warmup) & \n",
        "             \t\t\t\t\t\t\t    (probs2 > args.probs_threshold_warmup)\n",
        "                     \t)\n",
        "             \n",
        "\n",
        "             loss1_per_sample = criterion(out_src_cl1_1, label1)\n",
        "             src_loss1 =  torch.mean(loss1_per_sample) # torch.mean(loss1_per_sample[indices1]) #  \n",
        "             \n",
        "             loss2_per_sample = criterion(out_src_cl2_1, label1)\n",
        "             src_loss2 = torch.mean(loss2_per_sample) # torch.mean(loss2_per_sample[indices1]) # \n",
        "             \n",
        "             ins_loss = 0                  \n",
        "             kl_loss = 0\n",
        "             \n",
        "             count += (preds1 == label1).cpu().sum().item()\n",
        "             \n",
        "             loss = src_loss1 + src_loss2 \n",
        "             \n",
        "             if epoch == args.warmup_epochs - 1:                \n",
        "                correct_indices = indices1  \n",
        "                train_dataset.set_clean_data(idx1[correct_indices].detach().cpu().tolist(), \n",
        "                                             label1[correct_indices].detach().cpu().tolist())\n",
        "                #print(f'Length of clean dataset :{len(train_dataset.clean_data)}',file=logfile)\n",
        "                #print(f'Length of clean dataset :{len(train_dataset.clean_data)}')\n",
        "          else:\n",
        "             data2 = data2.to(args.device)\n",
        "             label2 =  label2.to(args.device)\n",
        "             feat2 = model(data2)\n",
        "             idx2 = idx2.to(args.device)\n",
        "             \n",
        "             # out_src_cl1 is for cl1 - out_src_cl2 is for cl2. out_src_cl{i}_1 is for clean data, out_src_cl{i}_2 is for messy data\n",
        "             # out_ins_cl_1 - is clean data into ins / out_ins_cl_2 is messy data into ins\n",
        "             out_ins_cl_2, probs_ins_cl_2 = ins_cl(feat2)\n",
        "             out_src_cl1_2, probs_src_cl1_2  = src_cl1(feat2) #messy out1\n",
        "             out_src_cl2_2, probs_src_cl2_2 = src_cl2(feat2)  #messy out2\n",
        "\n",
        "             src_loss1_per_sample = criterion(out_src_cl1_1, label1) \n",
        "             src_loss1 = torch.mean(src_loss1_per_sample)\n",
        "             \n",
        "             src_loss2_per_sample = criterion(out_src_cl2_1, label1) \n",
        "             src_loss2 = torch.mean(src_loss2_per_sample)\n",
        "             \n",
        "             ins_loss_per_sample_1 = criterion(out_ins_cl_1, idx1)  # from clean\n",
        "             ins_loss_1 = torch.mean(ins_loss_per_sample_1)  \n",
        "             ins_loss_per_sample_2 = criterion(out_ins_cl_2, idx2)  # from messy\n",
        "             ins_loss_2 = torch.mean(ins_loss_per_sample_2)\n",
        "             ins_loss = ins_loss_1 + ins_loss_2\n",
        "             \n",
        "             kl_loss1 = criterion_kl(torch.log(probs_src_cl1_2), probs_src_cl2_2)  # from messy out from 1 || 2\n",
        "             kl_loss2 = criterion_kl(torch.log(probs_src_cl2_2), probs_src_cl1_2)  # from messy out from 2 || 1           \n",
        "             kl_loss = kl_loss1 + kl_loss2\n",
        "             src_loss = src_loss1 + src_loss2\n",
        "             \n",
        "             if epoch>15:\n",
        "               a,b,c = .2,.5,.3\n",
        "             else:\n",
        "               a,b,c = .3,.3,.4\n",
        "             loss = a*src_loss +  b*kl_loss + c*ins_loss \n",
        "          \n",
        "          optimizer.zero_grad()   \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          #src_loss2 = 0\n",
        "          print(f\"Epoch/batch {epoch}/{i}\\tsrc_loss1:{src_loss1:.3f}\\tsrc_loss2:{src_loss2:.3f}\\tins_loss:{ins_loss:.3f}\\tkl_loss:{kl_loss:.3f}\",file=logfile) \n",
        "          \n",
        "      if epoch == args.warmup_epochs - 1:\n",
        "          train_dataset.set_phase(phase=2)   \n",
        "          \n",
        "      #Perform testing\n",
        "      if epoch % args.test_freq == 0:\n",
        "        model.eval()\n",
        "        src_cl1.eval()\n",
        "        src_cl2.eval()\n",
        "                \n",
        "        correct_cls_1 = 0.\n",
        "        correct_cls_2 = 0.\n",
        "        for i, (data, label) in enumerate(test_loader): \n",
        "              data = data.to(args.device)\n",
        "              label =  label.to(args.device)\n",
        "                                   \n",
        "              with torch.no_grad():\n",
        "                 feat = model(data)            \n",
        "                 _, probs_src_cl1 = src_cl1(feat)\n",
        "                 _, probs_src_cl2 = src_cl2(feat)\n",
        "                                               \n",
        "                 probs1, preds_cls_1 = torch.max(probs_src_cl1, dim = 1)\n",
        "                 probs2, preds_cls_2 = torch.max(probs_src_cl2, dim = 1)  \n",
        "                 correct_cls_1 += (preds_cls_1 == label).cpu().sum().item()\n",
        "                 correct_cls_2 += (preds_cls_2 == label).cpu().sum().item()          \n",
        "            \n",
        "        acc_cls_1 = correct_cls_1/len(test_dataset)\n",
        "        \n",
        "        acc_cls_2 = correct_cls_2/len(test_dataset)\n",
        "        \n",
        "        print(f\"Test: Epoch {epoch}\\tsrc_cls1_acc:{acc_cls_1:.4f}\\tsrc_cls2_acc:{acc_cls_2:.4f}\")\n",
        "        print(f\"Test: Epoch {epoch}\\tsrc_cls1_acc:{acc_cls_1:.4f}\\tsrc_cls2_acc:{acc_cls_2:.4f}\",file=logfile)\n",
        "        \n",
        "        if best_acc < acc_cls_1 or best_acc < acc_cls_2:\n",
        "           best_acc = max(acc_cls_1, acc_cls_2)\n",
        "           print(f'best_acc: {best_acc}')\n",
        "           \"\"\"torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'src_cl1_state_dict' : src_cl1.state_dict(),\n",
        "            'src_cl2_state_dict' : src_cl2.state_dict(),            \n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': criterion,\n",
        "            }, '/content/drive/MyDrive/Colab_Notebooks/mtech/Project/Logs/RAFDB/log_IDN_kl_run3_0.pth')\"\"\"\n",
        "    \n",
        "    \n",
        "    print(f\"\\n\\n \\t Best Test: Best_acc:{best_acc:.4f}. Sairam\",file=logfile)    \n",
        "    print(f\"\\n\\n \\t Best Test: Best_acc:{best_acc:.4f}. Sairam\")    \n",
        "    \n",
        "    return model, src_cl1, src_cl2, ins_cl, train_dataset \n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o7vhN0rF448",
        "outputId": "3a53faea-9e5e-4935-8c0d-3303100445e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warmup epochs: 8 - Noise: True,Noise Rate: /content/drive/MyDrive/Colab_Notebooks/mtech/Project/noise files/0.3noise_train.txt <_io.TextIOWrapper name='/content/drive/MyDrive/Colab_Notebooks/mtech/Project/Logs/IDN/RAFDB/filtering/0noise_allfiltered_1' mode='w' encoding='UTF-8'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1362: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: Epoch 0\tsrc_cls1_acc:0.5909\tsrc_cls2_acc:0.5932\n",
            "best_acc: 0.5932203389830508\n",
            "Test: Epoch 1\tsrc_cls1_acc:0.7216\tsrc_cls2_acc:0.7216\n",
            "best_acc: 0.7216427640156454\n",
            "Test: Epoch 2\tsrc_cls1_acc:0.7686\tsrc_cls2_acc:0.7696\n",
            "best_acc: 0.7695567144719687\n",
            "Test: Epoch 3\tsrc_cls1_acc:0.7806\tsrc_cls2_acc:0.7813\n",
            "best_acc: 0.7812907431551499\n",
            "Test: Epoch 4\tsrc_cls1_acc:0.7712\tsrc_cls2_acc:0.7715\n",
            "Test: Epoch 5\tsrc_cls1_acc:0.7826\tsrc_cls2_acc:0.7816\n",
            "best_acc: 0.7825945241199479\n",
            "Test: Epoch 6\tsrc_cls1_acc:0.7728\tsrc_cls2_acc:0.7728\n",
            "Test: Epoch 7\tsrc_cls1_acc:0.8008\tsrc_cls2_acc:0.7999\n",
            "best_acc: 0.8008474576271186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: Epoch 8\tsrc_cls1_acc:0.8204\tsrc_cls2_acc:0.8204\n",
            "best_acc: 0.8204041720990873\n",
            "Test: Epoch 9\tsrc_cls1_acc:0.8341\tsrc_cls2_acc:0.8341\n",
            "best_acc: 0.8340938722294654\n",
            "Test: Epoch 10\tsrc_cls1_acc:0.8259\tsrc_cls2_acc:0.8269\n",
            "Test: Epoch 11\tsrc_cls1_acc:0.8299\tsrc_cls2_acc:0.8302\n",
            "Test: Epoch 12\tsrc_cls1_acc:0.8318\tsrc_cls2_acc:0.8315\n",
            "Test: Epoch 13\tsrc_cls1_acc:0.8367\tsrc_cls2_acc:0.8367\n",
            "best_acc: 0.8367014341590613\n",
            "Test: Epoch 14\tsrc_cls1_acc:0.8276\tsrc_cls2_acc:0.8272\n",
            "Test: Epoch 15\tsrc_cls1_acc:0.8419\tsrc_cls2_acc:0.8422\n",
            "best_acc: 0.8422425032594524\n",
            "Test: Epoch 16\tsrc_cls1_acc:0.8396\tsrc_cls2_acc:0.8396\n",
            "Test: Epoch 17\tsrc_cls1_acc:0.8413\tsrc_cls2_acc:0.8419\n",
            "Test: Epoch 18\tsrc_cls1_acc:0.8380\tsrc_cls2_acc:0.8380\n",
            "Test: Epoch 19\tsrc_cls1_acc:0.8383\tsrc_cls2_acc:0.8377\n",
            "Test: Epoch 20\tsrc_cls1_acc:0.8432\tsrc_cls2_acc:0.8435\n",
            "best_acc: 0.8435462842242504\n",
            "Test: Epoch 21\tsrc_cls1_acc:0.8445\tsrc_cls2_acc:0.8442\n",
            "best_acc: 0.8445241199478487\n",
            "Test: Epoch 22\tsrc_cls1_acc:0.8435\tsrc_cls2_acc:0.8439\n",
            "Test: Epoch 23\tsrc_cls1_acc:0.8432\tsrc_cls2_acc:0.8435\n",
            "Test: Epoch 24\tsrc_cls1_acc:0.8406\tsrc_cls2_acc:0.8413\n",
            "Test: Epoch 25\tsrc_cls1_acc:0.8419\tsrc_cls2_acc:0.8416\n",
            "Test: Epoch 26\tsrc_cls1_acc:0.8426\tsrc_cls2_acc:0.8426\n",
            "Test: Epoch 27\tsrc_cls1_acc:0.8419\tsrc_cls2_acc:0.8419\n",
            "Test: Epoch 28\tsrc_cls1_acc:0.8429\tsrc_cls2_acc:0.8432\n",
            "Test: Epoch 29\tsrc_cls1_acc:0.8416\tsrc_cls2_acc:0.8419\n",
            "Test: Epoch 30\tsrc_cls1_acc:0.8416\tsrc_cls2_acc:0.8419\n",
            "Test: Epoch 31\tsrc_cls1_acc:0.8426\tsrc_cls2_acc:0.8426\n",
            "Test: Epoch 32\tsrc_cls1_acc:0.8432\tsrc_cls2_acc:0.8432\n",
            "Test: Epoch 33\tsrc_cls1_acc:0.8429\tsrc_cls2_acc:0.8429\n",
            "Test: Epoch 34\tsrc_cls1_acc:0.8432\tsrc_cls2_acc:0.8432\n",
            "Test: Epoch 35\tsrc_cls1_acc:0.8422\tsrc_cls2_acc:0.8426\n",
            "Test: Epoch 36\tsrc_cls1_acc:0.8422\tsrc_cls2_acc:0.8426\n",
            "Test: Epoch 37\tsrc_cls1_acc:0.8422\tsrc_cls2_acc:0.8426\n",
            "Test: Epoch 38\tsrc_cls1_acc:0.8426\tsrc_cls2_acc:0.8422\n",
            "Test: Epoch 39\tsrc_cls1_acc:0.8422\tsrc_cls2_acc:0.8426\n",
            "Test: Epoch 40\tsrc_cls1_acc:0.8422\tsrc_cls2_acc:0.8419\n",
            "Test: Epoch 41\tsrc_cls1_acc:0.8429\tsrc_cls2_acc:0.8426\n",
            "Test: Epoch 42\tsrc_cls1_acc:0.8429\tsrc_cls2_acc:0.8432\n",
            "Test: Epoch 43\tsrc_cls1_acc:0.8429\tsrc_cls2_acc:0.8429\n",
            "Test: Epoch 44\tsrc_cls1_acc:0.8432\tsrc_cls2_acc:0.8432\n",
            "Test: Epoch 45\tsrc_cls1_acc:0.8432\tsrc_cls2_acc:0.8429\n",
            "Test: Epoch 46\tsrc_cls1_acc:0.8429\tsrc_cls2_acc:0.8429\n",
            "\n",
            "\n",
            " \t Best Test: Best_acc:0.8445. Sairam\n"
          ]
        }
      ],
      "source": [
        "\n",
        "### Entire rafdb_datset file below\n",
        "\n",
        "class RafDataSet(data.Dataset):\n",
        "    def __init__(self, raf_path, noise_file, phase, noise = True, partition = 'train', transform = None, num_classes = 7):\n",
        "        self.phase = phase\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.raf_path = raf_path\n",
        "        self.clean_data = dict()\n",
        "        self.phase = 1 #pretraining \n",
        "        self.num_classes = num_classes\n",
        "        self.partition = partition\n",
        "        \n",
        "        NAME_COLUMN = 0\n",
        "        LABEL_COLUMN = 1\n",
        "        df_train_clean = pd.read_csv(os.path.join(self.raf_path, 'RAFDB/train_label.txt'), sep=' ', header=None)\n",
        "        df_train_noisy = pd.read_csv(os.path.join(self.raf_path, noise_file), sep=' ', header=None)\n",
        "        \n",
        "        df_test = pd.read_csv(os.path.join(self.raf_path, 'RAFDB/test_label.txt'), sep=' ', header=None)\n",
        "        if partition == 'train':\n",
        "            dataset_train_noisy = df_train_noisy[df_train_noisy[NAME_COLUMN].str.startswith('train')]\n",
        "            dataset_train_clean = df_train_clean[df_train_clean[NAME_COLUMN].str.startswith('train')]\n",
        "            self.clean_label = dataset_train_clean.iloc[:, LABEL_COLUMN].values - 1 # 0:Surprise, 1:Fear, 2:Disgust, 3:Happiness, 4:Sadness, 5:Anger, 6:Neutral\n",
        "            self.noisy_label = dataset_train_noisy.iloc[:, LABEL_COLUMN].values - 1 # 0:Surprise, 1:Fear, 2:Disgust, 3:Happiness, 4:Sadness, 5:Anger, 6:Neutral\n",
        "            if noise:\n",
        "              self.label = self.noisy_label  # if noise file used\n",
        "            else:\n",
        "              self.label = self.clean_label\n",
        "            file_names = dataset_train_noisy.iloc[:, NAME_COLUMN].values\n",
        "            #self.pseudo_probs1 = [0]*self.label.shape[0]\n",
        "            #self.pseudo_probs2 = [0]*self.label.shape[0]\n",
        "            self.noise_or_not = (self.noisy_label == self.clean_label) #By DG\n",
        "        else:             \n",
        "            dataset = df_test[df_test[NAME_COLUMN].str.startswith('test')]\n",
        "            self.label = dataset.iloc[:, LABEL_COLUMN].values - 1 # 0:Surprise, 1:Fear, 2:Disgust, 3:Happiness, 4:Sadness, 5:Anger, 6:Neutral            \n",
        "            file_names = dataset.iloc[:, NAME_COLUMN].values\n",
        "        \n",
        "        new_label = [] \n",
        "        \n",
        "        for label in self.label:\n",
        "            new_label.append(self.change_emotion_label_same_as_affectnet(label))\n",
        "            \n",
        "        self.label = new_label\n",
        "        self.pseudo_labels = []  \n",
        "        \n",
        "        self.file_paths = []\n",
        "        # use raf aligned images for training/testing\n",
        "        for f in file_names:\n",
        "            f = f.split(\".\")[0]\n",
        "            f = f +\"_aligned.jpg\"\n",
        "            working_directory = self.raf_path + 'RAFDB/aligned'\n",
        "            path = os.path.join(working_directory, f)\n",
        "            self.file_paths.append(path)\n",
        "        \n",
        "        self.pseudo_probs1 = torch.zeros((len(self.label), self.num_classes))\n",
        "        self.pseudo_probs2 = torch.zeros((len(self.label), self.num_classes))\n",
        "        \n",
        "        \n",
        "    def set_clean_data(self, indices, pseudo_labels):  # To be called after warmup period\n",
        "        self.clean_data.update(zip(indices, pseudo_labels))\n",
        "        \n",
        "    def set_probs(self, indices, probs1, probs2):\n",
        "        indices = indices.tolist()\n",
        "        for i in range(len(indices)):\n",
        "          self.pseudo_probs1[indices[i]] = probs1[i]\n",
        "          self.pseudo_probs2[indices[i]] = probs2[i]\n",
        "        \n",
        "    def set_phase(self, phase):\n",
        "        self.phase = phase\n",
        "        \n",
        "        \n",
        "    def change_emotion_label_same_as_affectnet(self, emo_to_return):\n",
        "        \"\"\"\n",
        "        Parse labels to make them compatible with AffectNet.  \n",
        "        #https://github.com/siqueira-hc/Efficient-Facial-Feature-Learning-with-Wide-Ensemble-based-Convolutional-Neural-Networks/blob/master/model/utils/udata.py\n",
        "        \"\"\"\n",
        "\n",
        "        if emo_to_return == 0:\n",
        "            emo_to_return = 3\n",
        "        elif emo_to_return == 1:\n",
        "            emo_to_return = 4\n",
        "        elif emo_to_return == 2:\n",
        "            emo_to_return = 5\n",
        "        elif emo_to_return == 3:\n",
        "            emo_to_return = 1\n",
        "        elif emo_to_return == 4:\n",
        "            emo_to_return = 2\n",
        "        elif emo_to_return == 5:\n",
        "            emo_to_return = 6\n",
        "        elif emo_to_return == 6:\n",
        "            emo_to_return = 0\n",
        "\n",
        "        return emo_to_return   \n",
        "         \n",
        "    def __len__(self):                   \n",
        "           return len(self.file_paths)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        if self.partition == 'train': \n",
        "          if self.phase == 1: #warm-up\n",
        "             label = self.label[idx]\n",
        "             path = self.file_paths[idx]\n",
        "             labeled = True   \n",
        "             image = cv2.imread(path)\n",
        "             image = image[:, :, ::-1] # BGR to RGB\n",
        "        \n",
        "             if self.transform is not None:\n",
        "                image =  self.transform(image)\n",
        "            \n",
        "             label = torch.tensor(label, dtype = torch.int64) \n",
        "             idx = torch.tensor(idx, dtype = torch.int64)  \n",
        "             return image, image, label, label, idx, idx, labeled, labeled   \n",
        "                \n",
        "          else:       #pseudo-labeling   \n",
        "             if idx in self.clean_data:\n",
        "               idx1 = idx\n",
        "               label1 = self.clean_data[idx1]                     \n",
        "               path1 = self.file_paths[idx1]\n",
        "               labeled1 = True\n",
        "             else:\n",
        "               idx1 = random.choice(list(self.clean_data.keys()))\n",
        "               label1 = self.clean_data[idx1]                     \n",
        "               path1 = self.file_paths[idx1]\n",
        "               labeled1 = True\n",
        "               \n",
        "             assigned_indices = set(self.clean_data.keys())\n",
        "             unassigned_indices = list(set(range(len(self))) - assigned_indices)\n",
        "             idx2 = random.choice(unassigned_indices)\n",
        "             label2 = self.label[idx2]\n",
        "             path2 = self.file_paths[idx2]\n",
        "             labeled2 = False     \n",
        "             \n",
        "             image1 = cv2.imread(path1)\n",
        "             image2 = cv2.imread(path2)\n",
        "             image1 = image1[:, :, ::-1] # BGR to RGB\n",
        "             image2 = image2[:, :, ::-1] # BGR to RGB\n",
        "        \n",
        "             if self.transform is not None:\n",
        "                image1 =  self.transform(image1)\n",
        "                image2 =  self.transform(image2)\n",
        "            \n",
        "             label1 = torch.tensor(label1, dtype = torch.int64) \n",
        "             idx1 = torch.tensor(idx1, dtype = torch.int64)  \n",
        "             label2 = torch.tensor(label2, dtype = torch.int64) \n",
        "             idx2 = torch.tensor(idx2, dtype = torch.int64)\n",
        "             \n",
        "             return image1, image2, label1, label2, idx1, idx2, labeled1, labeled2  \n",
        "             \n",
        "        else:     \n",
        "             label = self.label[idx]\n",
        "             path = self.file_paths[idx]\n",
        "             \n",
        "             image = cv2.imread(path)\n",
        "             \n",
        "             if self.transform is not None:\n",
        "                image =  self.transform(image)\n",
        "                \n",
        "             label = torch.tensor(label, dtype = torch.int64) \n",
        "                       \n",
        "             return image, label     \n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--base_model_lr', type=float, default=0.001)\n",
        "parser.add_argument('--src_lr', type=float, default=0.01)\n",
        "parser.add_argument('--ins_lr', type=float, default=0.01)\n",
        "\n",
        "parser.add_argument('--raf_path', type=str, default='/content/drive/MyDrive/Colab_Notebooks/mtech/Project/', help='Raf-DB dataset path.')   # Set path\n",
        "    \n",
        "parser.add_argument('--pretrained', type=str, default='/content/drive/MyDrive/Colab_Notebooks/mtech/Project/ijba_res18_naive.pth.tar',\n",
        "                        help='Pretrained weights')                  # Set path of pretrained model\n",
        "\n",
        "parser.add_argument('--resume', type=str, default='', help='Use FEC trained models')                     \n",
        "                        \n",
        "parser.add_argument('--noise_file', type=str, help='train_label.txt, 0.3noise_train.txt', default='/content/drive/MyDrive/Colab_Notebooks/mtech/Project/noise files/0.3noise_train.txt')  # How? and Set path\n",
        "\n",
        "parser.add_argument('--noise', type=bool, default=True)\n",
        "\n",
        "parser.add_argument('--epochs', type=int, default=47)\n",
        "\n",
        "parser.add_argument('--num_src_classes', type=int, default=7)\n",
        "\n",
        "parser.add_argument('--print_freq', type=int, default=30)\n",
        "\n",
        "parser.add_argument('--test_freq', type=int, default=1)\n",
        "\n",
        "parser.add_argument('--num_workers', type=int, default=4, help='how many subprocesses to use for data loading')\n",
        "\n",
        "parser.add_argument('--batch_size', type=int, default=128, help='batch_size')\n",
        "\n",
        "parser.add_argument('--warmup_epochs', type=int, default=8, help='Warmup epochs.')\n",
        "\n",
        "parser.add_argument('--base_model_wd', type=float, default=1e-6)\n",
        "\n",
        "parser.add_argument('--other_wd', type=float, default=1e-4)\n",
        "\n",
        "parser.add_argument('--momentum', type=float, default=0.9)\n",
        "\n",
        "parser.add_argument('--probs_threshold_warmup', type=float, default=0.02)\n",
        "\n",
        "parser.add_argument('--probs_threshold', type=float, default=0.94)\n",
        "\n",
        "args = parser.parse_args(\" \".split())\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   args.device = 'cuda'\n",
        "else:\n",
        "   args.device = 'cpu' \n",
        "\n",
        "def main(args):\n",
        "\n",
        "  file_str = f'/content/drive/MyDrive/Colab_Notebooks/mtech/Project/Logs/IDN/RAFDB/filtering/0noise_allfiltered_{1}'\n",
        "  logfile = open(file_str,'w')\n",
        "  #print(f\"Warmup epochs: {args.warmup_epochs} - Noise: {args.noise},Noise Rate: {args.noise_file}\")\n",
        "  print(f\"Warmup epochs: {args.warmup_epochs} - Noise: {args.noise},Noise Rate: {args.noise_file}\",logfile)\n",
        "  train_transform = transforms.Compose([\n",
        "          transforms.ToPILImage(),\n",
        "          transforms.RandomHorizontalFlip(p=0.5), transforms.RandomApply([transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.25),\n",
        "                  transforms.RandomAffine(degrees=0, translate=(.1, .1), scale=(1.0, 1.25),resample=Image.BILINEAR)],p=0.5), \n",
        "                \n",
        "          transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "  test_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                        transforms.Resize((224, 224)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                            std=[0.229, 0.224, 0.225])])\n",
        "                                   \n",
        "                                   \n",
        "  train_dataset = RafDataSet(raf_path=args.raf_path, noise_file = args.noise_file, phase = 1, noise = args.noise, partition = 'train', transform = train_transform, num_classes = args.num_src_classes)\n",
        "  test_dataset = RafDataSet(raf_path=args.raf_path, noise_file = args.noise_file, phase = 1, noise = args.noise, partition = 'test', transform = test_transform, num_classes =  args.num_src_classes)\n",
        "  args.num_ins_classes = len(train_dataset) \n",
        "  model, src_cl1, src_cl2, ins_cl, train_dataset = train(args, train_dataset, test_dataset, logfile)\n",
        "                                                       \n",
        "    \n",
        "if __name__=='__main__':\n",
        "   main(args)                                           \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Results under 30% Synthetic noise on RAFDB: 84.45%"
      ],
      "metadata": {
        "id": "rs1GNZUMoWbQ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IEEE_SPL_Rafdb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0AAggmFQK6mwao9mVTp24",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}